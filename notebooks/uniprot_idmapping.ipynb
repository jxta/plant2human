{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "import requests\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Tuple\n",
    "from unipressed import IdMappingClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "gene_id_tsv = \"../test/zea_mays_test/zea_mays_random_gene_list.tsv\"\n",
    "query_db = \"Ensembl_Genomes\"\n",
    "target_db = \"UniProtKB\"\n",
    "json_dir = \"zea_mays_random_gene_afinfo\"\n",
    "data_url = \"cifUrl\" # or \"pdbUrl\", \"bcifUrl\", \"paeImageUrl\", \"paeDocUrl\"\n",
    "structure_dir = \"zea_mays_random_gene_mmcif\"\n",
    "id_mapping_all_file = \"zea_mays_random_gene_idmapping_all.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. UniProt ID mapping step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "Function"
    ]
   },
   "outputs": [],
   "source": [
    "def chunk_list(lst: List, chunk_size: int) -> List[List]:\n",
    "    \"\"\"Split a gene list into chunks\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_id_mapping(from_db: str, to_db: str, ids: List[str], chunk_size: int = 100) -> Tuple[pl.DataFrame, List[str]]:\n",
    "    \"\"\"function for batch id mapping\"\"\"\n",
    "    all_results = []\n",
    "    all_unmapped = []\n",
    "    chunked_ids = chunk_list(ids, chunk_size)\n",
    "\n",
    "    for i, chunk in enumerate(chunked_ids):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunked_ids)}...\")\n",
    "        \n",
    "        # create request and run\n",
    "        request = IdMappingClient.submit(source=from_db, dest=to_db, ids=chunk)\n",
    "        \n",
    "        # process results\n",
    "        chunk_results = list(request.each_result())\n",
    "        mapped_results = [{\"from\": item[\"from\"], \"to\": item[\"to\"]} for item in chunk_results]\n",
    "        all_results.extend(mapped_results)\n",
    "\n",
    "        # record unmapped ids\n",
    "        mapped_ids = set(item[\"from\"] for item in mapped_results)\n",
    "        unmapped = [id for id in chunk if id not in mapped_ids]\n",
    "        all_unmapped.extend(unmapped)\n",
    "\n",
    "        # avoid API rate limit\n",
    "        time.sleep(3)\n",
    "\n",
    "    # convert results to DataFrame\n",
    "    final_df = pl.DataFrame(all_results)\n",
    "    return final_df, all_unmapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids = pl.read_csv(\n",
    "    gene_id_tsv,\n",
    "    separator='\\t'\n",
    ").get_column(\"From\").to_list()\n",
    "\n",
    "mapped_df, unmapped_ids = batch_id_mapping(\n",
    "    query_db,\n",
    "    target_db,\n",
    "    gene_ids\n",
    ")\n",
    "\n",
    "display(mapped_df)\n",
    "display(unmapped_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_df2 = mapped_df.rename(\n",
    "    {\n",
    "        \"from\": \"From\",\n",
    "        \"to\": \"UniProt Accession\"\n",
    "    }\n",
    ")\n",
    "\n",
    "display(mapped_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapped_df.is_empty())\n",
    "print(len(unmapped_ids) == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UniProt re-mapping \n",
    "\n",
    "## 3. Concatenate the two dataframes\n",
    "\n",
    "## 4. AlphaFoldDB metadata JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "function"
    ]
   },
   "outputs": [],
   "source": [
    "def fetch_uniprot_data(ensembl_ids: List[str]) -> pl.DataFrame:\n",
    "    results = []\n",
    "\n",
    "    for id in ensembl_ids:\n",
    "        print(f\"Processing {id}...\")\n",
    "        url = (\n",
    "            f\"https://rest.uniprot.org/uniprotkb/search?\"\n",
    "            f\"query=gene:{id}&format=json\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "            for item in data.get('results', []):\n",
    "                primary_accession = item.get('primaryAccession', '')\n",
    "                secondary_accessions = item.get('secondaryAccessions', [])\n",
    "                all_accessions = [primary_accession] + secondary_accessions\n",
    "                \n",
    "                for accession in all_accessions:\n",
    "                    entry = {\n",
    "                        \"From\": id,\n",
    "                        \"UniProt Accession\": accession\n",
    "                    }\n",
    "                    \n",
    "                    # Check if the accession is a match for the gene \n",
    "                    # (e.g. Os03g0293000 matches OrderedLocusNames)\n",
    "                    match_found = False\n",
    "                    for gene in item.get('genes', []):\n",
    "                        for locus in gene.get('orderedLocusNames', []):\n",
    "                            if locus.get('value', '') == id:\n",
    "                                match_found = True\n",
    "                                break\n",
    "                        if match_found:\n",
    "                            break\n",
    "                    \n",
    "                    if match_found:\n",
    "                        results.append(entry)\n",
    "        else:\n",
    "            print(f\"Error fetching data for {id}: {response.status_code}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "    return pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_af_json(dataframe: pl.DataFrame, target_dir: str):\n",
    "    \"\"\"\n",
    "    Get JSON file from AlphaFoldDB\n",
    "    \"\"\"\n",
    "    pathlib.Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for row in dataframe.iter_rows():\n",
    "        gene_id = row[0]\n",
    "        uniprot_id = row[1]\n",
    "        \n",
    "        json_file_name = pathlib.Path(target_dir) / f\"{gene_id}_{uniprot_id}_info.json\"\n",
    "        \n",
    "        if json_file_name.exists():\n",
    "            message_1 = f\"{json_file_name} already exists\"\n",
    "            print(message_1)\n",
    "            continue\n",
    "        \n",
    "        request_url = f\"https://alphafold.ebi.ac.uk/api/prediction/{uniprot_id}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(request_url, headers={\"Accept\": \"application/json\"}, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if response.text:\n",
    "                data = json.loads(response.text) # parse json\n",
    "                if isinstance(data, list) and len(data) > 0:\n",
    "                    message_2 = f\"AlphaFold ID {uniprot_id} found in AlphaFold\"\n",
    "                    print(message_2)\n",
    "                    with open(json_file_name, 'w') as f:\n",
    "                        json.dump(data[0], f, indent=4)\n",
    "                else:\n",
    "                    message_3 = f\"AlphaFold ID {uniprot_id} not found in AlphaFold\"\n",
    "                    print(message_3)\n",
    "            else:\n",
    "                message_4 = f\"Empty response for AlphaFold ID {uniprot_id}\"\n",
    "                print(message_4)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            message_5 = f\"Request failed: {e}\"\n",
    "            print(message_5)\n",
    "            message_6 = f\"AlphaFold ID {uniprot_id} not found in AlphaFold\"\n",
    "            print(message_6)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmapped_df = fetch_uniprot_data(unmapped_ids)\n",
    "display(unmapped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(unmapped_ids) > 0:\n",
    "    unmapped_df = fetch_uniprot_data(unmapped_ids)\n",
    "    display(unmapped_df)\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    if not unmapped_df.is_empty():\n",
    "        id_mapping_df = pl.concat(\n",
    "            [\n",
    "                mapped_df2,\n",
    "                unmapped_df\n",
    "            ],\n",
    "            how=\"vertical_relaxed\"\n",
    "        ).sort(\n",
    "            by=\"From\",\n",
    "            descending=False\n",
    "        )\n",
    "        display(id_mapping_df)\n",
    "        # Get AlphaFold metadata JSON files\n",
    "        get_af_json(id_mapping_df, json_dir)\n",
    "    else:\n",
    "        print(\"unmapped dataframe is empty, skipping get_af_json.\")\n",
    "        get_af_json(mapped_df2, json_dir)\n",
    "else:\n",
    "    print(\"unmapped_ids is empty, skipping fetch_uniprot_data.\")\n",
    "    get_af_json(mapped_df2, json_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download CIF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cif_file(json_dir_path: str, output_dir_path: str):\n",
    "    \"\"\"\n",
    "    Get CIF file from JSON files retrieved from AlphaFoldDB\n",
    "    \n",
    "    Args:\n",
    "        json_dir_path: Directory containing AlphaFold JSON metadata files\n",
    "        output_dir_path: Directory to save downloaded CIF files\n",
    "    \"\"\"\n",
    "    pathlib.Path(output_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for json_file in pathlib.Path(json_dir_path).glob(\"*.json\"):\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                cif_url = data.get(data_url)\n",
    "                \n",
    "                if not cif_url:\n",
    "                    print(f\"No {data_url} found in {json_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Extract filename from URL and create output path\n",
    "                cif_filename = pathlib.Path(cif_url).name\n",
    "                output_file = pathlib.Path(output_dir_path) / cif_filename\n",
    "                \n",
    "                # Skip if file already exists\n",
    "                if output_file.exists():\n",
    "                    print(f\"{output_file} already exists\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Downloading {cif_url}\")\n",
    "                response = requests.get(cif_url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Save CIF file\n",
    "                output_file.write_bytes(response.content)\n",
    "                print(f\"Saved {output_file}\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON file: {json_file}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error downloading CIF file: {e}\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cif_file(json_dir, structure_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "if len(unmapped_ids) > 0 and not unmapped_df.is_empty():\n",
    "    id_mapping_df.write_csv(id_mapping_all_file, separator=\"\\t\")\n",
    "\n",
    "elif unmapped_df.is_empty():\n",
    "    print(\"re-mapping process is skipped, mapping results are saved in id_mapping_df.write_csv.\")\n",
    "    mapped_df2.write_csv(id_mapping_all_file, separator=\"\\t\")\n",
    "\n",
    "else:\n",
    "    print(\"unmapped_ids is empty, skipping re-mapping process.\")\n",
    "    mapped_df2.write_csv(id_mapping_all_file, separator=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
